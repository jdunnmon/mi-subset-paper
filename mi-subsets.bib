% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chilamkurthy2018-op,
  title    = "Deep learning algorithms for detection of critical findings in
              head {CT} scans: a retrospective study",
  author   = "Chilamkurthy, Sasank and Ghosh, Rohit and Tanamala, Swetha and
              Biviji, Mustafa and Campeau, Norbert G and Venugopal, Vasantha
              Kumar and Mahajan, Vidur and Rao, Pooja and Warier, Prashant",
  abstract = "BACKGROUND: Non-contrast head CT scan is the current standard for
              initial imaging of patients with head trauma or stroke symptoms.
              We aimed to develop and validate a set of deep learning
              algorithms for automated detection of the following key findings
              from these scans: intracranial haemorrhage and its types (ie,
              intraparenchymal, intraventricular, subdural, extradural, and
              subarachnoid); calvarial fractures; midline shift; and mass
              effect. METHODS: We retrospectively collected a dataset
              containing 313 318 head CT scans together with their clinical
              reports from around 20 centres in India between Jan 1, 2011, and
              June 1, 2017. A randomly selected part of this dataset (Qure25k
              dataset) was used for validation and the rest was used to develop
              algorithms. An additional validation dataset (CQ500 dataset) was
              collected in two batches from centres that were different from
              those used for the development and Qure25k datasets. We excluded
              postoperative scans and scans of patients younger than 7 years.
              The original clinical radiology report and consensus of three
              independent radiologists were considered as gold standard for the
              Qure25k and CQ500 datasets, respectively. Areas under the
              receiver operating characteristic curves (AUCs) were primarily
              used to assess the algorithms. FINDINGS: The Qure25k dataset
              contained 21 095 scans (mean age 43 years; 9030 [43\%] female
              patients), and the CQ500 dataset consisted of 214 scans in the
              first batch (mean age 43 years; 94 [44\%] female patients) and
              277 scans in the second batch (mean age 52 years; 84 [30\%]
              female patients). On the Qure25k dataset, the algorithms achieved
              an AUC of 0·92 (95\% CI 0·91-0·93) for detecting intracranial
              haemorrhage (0·90 [0·89-0·91] for intraparenchymal, 0·96
              [0·94-0·97] for intraventricular, 0·92 [0·90-0·93] for subdural,
              0·93 [0·91-0·95] for extradural, and 0·90 [0·89-0·92] for
              subarachnoid). On the CQ500 dataset, AUC was 0·94 (0·92-0·97) for
              intracranial haemorrhage (0·95 [0·93-0·98], 0·93 [0·87-1·00],
              0·95 [0·91-0·99], 0·97 [0·91-1·00], and 0·96 [0·92-0·99],
              respectively). AUCs on the Qure25k dataset were 0·92 (0·91-0·94)
              for calvarial fractures, 0·93 (0·91-0·94) for midline shift, and
              0·86 (0·85-0·87) for mass effect, while AUCs on the CQ500 dataset
              were 0·96 (0·92-1·00), 0·97 (0·94-1·00), and 0·92 (0·89-0·95),
              respectively. INTERPRETATION: Our results show that deep learning
              algorithms can accurately identify head CT scan abnormalities
              requiring urgent attention, opening up the possibility to use
              these algorithms to automate the triage process. FUNDING:
              Qure.ai.",
  journal  = "Lancet",
  volume   =  392,
  number   =  10162,
  pages    = "2388--2396",
  month    =  dec,
  year     =  2018,
  language = "en"
}

@ARTICLE{Esteva2017-if,
  title    = "Dermatologist-level classification of skin cancer with deep
              neural networks",
  author   = "Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko,
              Justin and Swetter, Susan M and Blau, Helen M and Thrun,
              Sebastian",
  abstract = "Skin cancer, the most common human malignancy, is primarily
              diagnosed visually, beginning with an initial clinical screening
              and followed potentially by dermoscopic analysis, a biopsy and
              histopathological examination. Automated classification of skin
              lesions using images is a challenging task owing to the
              fine-grained variability in the appearance of skin lesions. Deep
              convolutional neural networks (CNNs) show potential for general
              and highly variable tasks across many fine-grained object
              categories. Here we demonstrate classification of skin lesions
              using a single CNN, trained end-to-end from images directly,
              using only pixels and disease labels as inputs. We train a CNN
              using a dataset of 129,450 clinical images-two orders of
              magnitude larger than previous datasets-consisting of 2,032
              different diseases. We test its performance against 21
              board-certified dermatologists on biopsy-proven clinical images
              with two critical binary classification use cases: keratinocyte
              carcinomas versus benign seborrheic keratoses; and malignant
              melanomas versus benign nevi. The first case represents the
              identification of the most common cancers, the second represents
              the identification of the deadliest skin cancer. The CNN achieves
              performance on par with all tested experts across both tasks,
              demonstrating an artificial intelligence capable of classifying
              skin cancer with a level of competence comparable to
              dermatologists. Outfitted with deep neural networks, mobile
              devices can potentially extend the reach of dermatologists
              outside of the clinic. It is projected that 6.3 billion
              smartphone subscriptions will exist by the year 2021 (ref. 13)
              and can therefore potentially provide low-cost universal access
              to vital diagnostic care.",
  journal  = "Nature",
  volume   =  542,
  number   =  7639,
  pages    = "115--118",
  month    =  feb,
  year     =  2017,
  language = "en"
}

@ARTICLE{Zech2018-xq,
  title         = "Confounding variables can degrade generalization performance
                   of radiological deep learning models",
  author        = "Zech, John R and Badgeley, Marcus A and Liu, Manway and
                   Costa, Anthony B and Titano, Joseph J and Oermann, Eric K",
  abstract      = "Early results in using convolutional neural networks (CNNs)
                   on x-rays to diagnose disease have been promising, but it
                   has not yet been shown that models trained on x-rays from
                   one hospital or one group of hospitals will work equally
                   well at different hospitals. Before these tools are used for
                   computer-aided diagnosis in real-world clinical settings, we
                   must verify their ability to generalize across a variety of
                   hospital systems. A cross-sectional design was used to train
                   and evaluate pneumonia screening CNNs on 158,323 chest
                   x-rays from NIH (n=112,120 from 30,805 patients), Mount
                   Sinai (42,396 from 12,904 patients), and Indiana (n=3,807
                   from 3,683 patients). In 3 / 5 natural comparisons,
                   performance on chest x-rays from outside hospitals was
                   significantly lower than on held-out x-rays from the
                   original hospital systems. CNNs were able to detect where an
                   x-ray was acquired (hospital system, hospital department)
                   with extremely high accuracy and calibrate predictions
                   accordingly. The performance of CNNs in diagnosing diseases
                   on x-rays may reflect not only their ability to identify
                   disease-specific imaging findings on x-rays, but also their
                   ability to exploit confounding information. Estimates of CNN
                   performance based on test data from hospital systems used
                   for model training may overstate their likely real-world
                   performance.",
  month         =  jul,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1807.00431"
}

@ARTICLE{Haenssle2018-vw,
  title     = "Man against machine: diagnostic performance of a deep learning
               convolutional neural network for dermoscopic melanoma
               recognition in comparison to 58 dermatologists",
  author    = "Haenssle, Holger A and Fink, Christine and Schneiderbauer, R and
               Toberer, Ferdinand and Buhl, Timo and Blum, A and Kalloo, A and
               Hassen, A Ben Hadj and Thomas, L and Enk, A and {Others}",
  journal   = "Ann. Oncol.",
  publisher = "Oxford University Press",
  volume    =  29,
  number    =  8,
  pages     = "1836--1842",
  year      =  2018
}

@MISC{Zech_undated-cw,
  title       = "reproduce-chexnet",
  author      = "Zech, John",
  abstract    = "Reproduce CheXNet. Contribute to jrzech/reproduce-chexnet
                 development by creating an account on GitHub.",
  institution = "Github"
}

@ARTICLE{Selbst2017-gz,
  title     = "Disparate impact in big data policing",
  author    = "Selbst, Andrew D",
  journal   = "Ga. L. Rev.",
  publisher = "HeinOnline",
  volume    =  52,
  pages     = "109",
  year      =  2017
}

@ARTICLE{Mazurowski2008-cq,
  title    = "Training neural network classifiers for medical decision making:
              the effects of imbalanced datasets on classification performance",
  author   = "Mazurowski, Maciej A and Habas, Piotr A and Zurada, Jacek M and
              Lo, Joseph Y and Baker, Jay A and Tourassi, Georgia D",
  abstract = "This study investigates the effect of class imbalance in training
              data when developing neural network classifiers for
              computer-aided medical diagnosis. The investigation is performed
              in the presence of other characteristics that are typical among
              medical data, namely small training sample size, large number of
              features, and correlations between features. Two methods of
              neural network training are explored: classical backpropagation
              (BP) and particle swarm optimization (PSO) with clinically
              relevant training criteria. An experimental study is performed
              using simulated data and the conclusions are further validated on
              real clinical data for breast cancer diagnosis. The results show
              that classifier performance deteriorates with even modest class
              imbalance in the training data. Further, it is shown that BP is
              generally preferable over PSO for imbalanced training data
              especially with small data sample and large number of features.
              Finally, it is shown that there is no clear preference between
              oversampling and no compensation approach and some guidance is
              provided regarding a proper selection.",
  journal  = "Neural Netw.",
  volume   =  21,
  number   = "2-3",
  pages    = "427--436",
  month    =  mar,
  year     =  2008,
  language = "en"
}

@MISC{Winkler2019-fw,
  title   = "Association Between Surgical Skin Markings in Dermoscopic Images
             and Diagnostic Performance of a Deep Learning Convolutional Neural
             Network for Melanoma Recognition",
  author  = "Winkler, Julia K and Fink, Christine and Toberer, Ferdinand and
             Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer
             and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz,
             Wilhelm and Haenssle, Holger A",
  journal = "JAMA Dermatology",
  year    =  2019
}

@ARTICLE{Irvin2019-ho,
  title   = "Chexpert: A large chest radiograph dataset with uncertainty labels
             and expert comparison",
  author  = "Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan
             and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik
             and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and
             {Others}",
  journal = "arXiv preprint arXiv:1901. 07031",
  year    =  2019
}

@INPROCEEDINGS{Wang2017-vm,
  title       = "Chestx-ray8: Hospital-scale chest x-ray database and
                 benchmarks on weakly-supervised classification and
                 localization of common thorax diseases",
  booktitle   = "Computer Vision and Pattern Recognition ({CVPR)}, 2017 {IEEE}
                 Conference on",
  author      = "Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and
                 Bagheri, Mohammadhadi and Summers, Ronald M",
  pages       = "3462--3471",
  institution = "IEEE",
  year        =  2017
}

@ARTICLE{Rajpurkar2018-gc,
  title    = "Deep learning for chest radiograph diagnosis: A retrospective
              comparison of the {CheXNeXt} algorithm to practicing radiologists",
  author   = "Rajpurkar, Pranav and Irvin, Jeremy and Ball, Robyn L and Zhu,
              Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and
              Ding, Daisy and Bagul, Aarti and Langlotz, Curtis P and Patel,
              Bhavik N and Yeom, Kristen W and Shpanskaya, Katie and
              Blankenberg, Francis G and Seekins, Jayne and Amrhein, Timothy J
              and Mong, David A and Halabi, Safwan S and Zucker, Evan J and Ng,
              Andrew Y and Lungren, Matthew P",
  abstract = "BACKGROUND: Chest radiograph interpretation is critical for the
              detection of thoracic diseases, including tuberculosis and lung
              cancer, which affect millions of people worldwide each year. This
              time-consuming task typically requires expert radiologists to
              read the images, leading to fatigue-based diagnostic error and
              lack of diagnostic expertise in areas of the world where
              radiologists are not available. Recently, deep learning
              approaches have been able to achieve expert-level performance in
              medical image interpretation tasks, powered by large network
              architectures and fueled by the emergence of large labeled
              datasets. The purpose of this study is to investigate the
              performance of a deep learning algorithm on the detection of
              pathologies in chest radiographs compared with practicing
              radiologists. METHODS AND FINDINGS: We developed CheXNeXt, a
              convolutional neural network to concurrently detect the presence
              of 14 different pathologies, including pneumonia, pleural
              effusion, pulmonary masses, and nodules in frontal-view chest
              radiographs. CheXNeXt was trained and internally validated on the
              ChestX-ray8 dataset, with a held-out validation set consisting of
              420 images, sampled to contain at least 50 cases of each of the
              original pathology labels. On this validation set, the majority
              vote of a panel of 3 board-certified cardiothoracic specialist
              radiologists served as reference standard. We compared CheXNeXt's
              discriminative performance on the validation set to the
              performance of 9 radiologists using the area under the receiver
              operating characteristic curve (AUC). The radiologists included 6
              board-certified radiologists (average experience 12 years, range
              4-28 years) and 3 senior radiology residents, from 3 academic
              institutions. We found that CheXNeXt achieved radiologist-level
              performance on 11 pathologies and did not achieve
              radiologist-level performance on 3 pathologies. The radiologists
              achieved statistically significantly higher AUC performance on
              cardiomegaly, emphysema, and hiatal hernia, with AUCs of 0.888
              (95\% confidence interval [CI] 0.863-0.910), 0.911 (95\% CI
              0.866-0.947), and 0.985 (95\% CI 0.974-0.991), respectively,
              whereas CheXNeXt's AUCs were 0.831 (95\% CI 0.790-0.870), 0.704
              (95\% CI 0.567-0.833), and 0.851 (95\% CI 0.785-0.909),
              respectively. CheXNeXt performed better than radiologists in
              detecting atelectasis, with an AUC of 0.862 (95\% CI
              0.825-0.895), statistically significantly higher than
              radiologists' AUC of 0.808 (95\% CI 0.777-0.838); there were no
              statistically significant differences in AUCs for the other 10
              pathologies. The average time to interpret the 420 images in the
              validation set was substantially longer for the radiologists (240
              minutes) than for CheXNeXt (1.5 minutes). The main limitations of
              our study are that neither CheXNeXt nor the radiologists were
              permitted to use patient history or review prior examinations and
              that evaluation was limited to a dataset from a single
              institution. CONCLUSIONS: In this study, we developed and
              validated a deep learning algorithm that classified clinically
              important abnormalities in chest radiographs at a performance
              level comparable to practicing radiologists. Once tested
              prospectively in clinical settings, the algorithm could have the
              potential to expand patient access to chest radiograph
              diagnostics.",
  journal  = "PLoS Med.",
  volume   =  15,
  number   =  11,
  pages    = "e1002686",
  month    =  nov,
  year     =  2018,
  language = "en"
}

@ARTICLE{Oakden-Rayner2019-yi,
  title   = "Exploring large scale public medical image datasets",
  author  = "Oakden-Rayner, Luke",
  journal = "arXiv",
  month   =  jul,
  year    =  2019
}

@ARTICLE{Mahajan2019-yi,
title = "The Algorithmic Audit: Working with Vendors to Validate Radiology-AI Algorithms - How We Do It"
author="Mahajan, Vidur and Venugopal, Vasanthakumar and Gaur, Saumya and Gupta, Salil and Murugavel, Murali and Mahajan, Harsh"
journal="viXra",
month=jul,
year = 2019
}

@ARTICLE{Rajpurkar2017-rc,
  title         = "{MURA}: Large Dataset for Abnormality Detection in
                   Musculoskeletal Radiographs",
  author        = "Rajpurkar, Pranav and Irvin, Jeremy and Bagul, Aarti and
                   Ding, Daisy and Duan, Tony and Mehta, Hershel and Yang,
                   Brandon and Zhu, Kaylie and Laird, Dillon and Ball, Robyn L
                   and Langlotz, Curtis and Shpanskaya, Katie and Lungren,
                   Matthew P and Ng, Andrew Y",
  abstract      = "We introduce MURA, a large dataset of musculoskeletal
                   radiographs containing 40,561 images from 14,863 studies,
                   where each study is manually labeled by radiologists as
                   either normal or abnormal. To evaluate models robustly and
                   to get an estimate of radiologist performance, we collect
                   additional labels from six board-certified Stanford
                   radiologists on the test set, consisting of 207
                   musculoskeletal studies. On this test set, the majority vote
                   of a group of three radiologists serves as gold standard. We
                   train a 169-layer DenseNet baseline model to detect and
                   localize abnormalities. Our model achieves an AUROC of
                   0.929, with an operating point of 0.815 sensitivity and
                   0.887 specificity. We compare our model and radiologists on
                   the Cohen's kappa statistic, which expresses the agreement
                   of our model and of each radiologist with the gold standard.
                   Model performance is comparable to the best radiologist
                   performance in detecting abnormalities on finger and wrist
                   studies. However, model performance is lower than best
                   radiologist performance in detecting abnormalities on elbow,
                   forearm, hand, humerus, and shoulder studies. We believe
                   that the task is a good challenge for future research. To
                   encourage advances, we have made our dataset freely
                   available at
                   https://stanfordmlgroup.github.io/competitions/mura .",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "physics.med-ph",
  eprint        = "1712.06957"
}

@ARTICLE{Agniel2018-qp,
  title    = "Biases in electronic health record data due to processes within
              the healthcare system: retrospective observational study",
  author   = "Agniel, Denis and Kohane, Isaac S and Weber, Griffin M",
  abstract = "OBJECTIVE: To evaluate on a large scale, across 272 common types
              of laboratory tests, the impact of healthcare processes on the
              predictive value of electronic health record (EHR) data. DESIGN:
              Retrospective observational study. SETTING: Two large hospitals
              in Boston, Massachusetts, with inpatient, emergency, and
              ambulatory care. PARTICIPANTS: All 669 452 patients treated at
              the two hospitals over one year between 2005 and 2006. MAIN
              OUTCOME MEASURES: The relative predictive accuracy of each
              laboratory test for three year survival, using the time of the
              day, day of the week, and ordering frequency of the test,
              compared to the value of the test result. RESULTS: The presence
              of a laboratory test order, regardless of any other information
              about the test result, has a significant association (P<0.001)
              with the odds of survival in 233 of 272 (86\%) tests. Data about
              the timing of when laboratory tests were ordered were more
              accurate than the test results in predicting survival in 118 of
              174 tests (68\%). CONCLUSIONS: Healthcare processes must be
              addressed and accounted for in analysis of observational health
              data. Without careful consideration to context, EHR data are
              unsuitable for many research questions. However, if explicitly
              modeled, the same processes that make EHR data complex can be
              leveraged to gain insight into patients' state of health.",
  journal  = "BMJ",
  volume   =  361,
  pages    = "k1479",
  month    =  apr,
  year     =  2018,
  language = "en"
}

@INCOLLECTION{Ratner2017-td,
  title     = "Learning to Compose {Domain-Specific} Transformations for Data
               Augmentation",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Ratner, Alexander J and Ehrenberg, Henry and Hussain, Zeshan and
               Dunnmon, Jared and R{\'e}, Christopher",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "3236--3246",
  year      =  2017
}

@ARTICLE{Gulshan2016-we,
  title    = "Development and Validation of a Deep Learning Algorithm for
              Detection of Diabetic Retinopathy in Retinal Fundus Photographs",
  author   = "Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin
              C and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan,
              Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge
              and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C and Mega,
              Jessica L and Webster, Dale R",
  abstract = "Importance: Deep learning is a family of computational methods
              that allow an algorithm to program itself by learning from a
              large set of examples that demonstrate the desired behavior,
              removing the need to specify rules explicitly. Application of
              these methods to medical imaging requires further assessment and
              validation. Objective: To apply deep learning to create an
              algorithm for automated detection of diabetic retinopathy and
              diabetic macular edema in retinal fundus photographs. Design and
              Setting: A specific type of neural network optimized for image
              classification called a deep convolutional neural network was
              trained using a retrospective development data set of 128 175
              retinal images, which were graded 3 to 7 times for diabetic
              retinopathy, diabetic macular edema, and image gradability by a
              panel of 54 US licensed ophthalmologists and ophthalmology senior
              residents between May and December 2015. The resultant algorithm
              was validated in January and February 2016 using 2 separate data
              sets, both graded by at least 7 US board-certified
              ophthalmologists with high intragrader consistency. Exposure:
              Deep learning-trained algorithm. Main Outcomes and Measures: The
              sensitivity and specificity of the algorithm for detecting
              referable diabetic retinopathy (RDR), defined as moderate and
              worse diabetic retinopathy, referable diabetic macular edema, or
              both, were generated based on the reference standard of the
              majority decision of the ophthalmologist panel. The algorithm was
              evaluated at 2 operating points selected from the development
              set, one selected for high specificity and another for high
              sensitivity. Results: The EyePACS-1 data set consisted of 9963
              images from 4997 patients (mean age, 54.4 years; 62.2\% women;
              prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the
              Messidor-2 data set had 1748 images from 874 patients (mean age,
              57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully
              gradable images [14.6\%]). For detecting RDR, the algorithm had
              an area under the receiver operating curve of 0.991 (95\% CI,
              0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for
              Messidor-2. Using the first operating cut point with high
              specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI,
              87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI,
              97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\%
              CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI,
              97.7\%-99.1\%). Using a second operating point with high
              sensitivity in the development set, for EyePACS-1 the sensitivity
              was 97.5\% and specificity was 93.4\% and for Messidor-2 the
              sensitivity was 96.1\% and specificity was 93.9\%. Conclusions
              and Relevance: In this evaluation of retinal fundus photographs
              from adults with diabetes, an algorithm based on deep machine
              learning had high sensitivity and specificity for detecting
              referable diabetic retinopathy. Further research is necessary to
              determine the feasibility of applying this algorithm in the
              clinical setting and to determine whether use of the algorithm
              could lead to improved care and outcomes compared with current
              ophthalmologic assessment.",
  journal  = "JAMA",
  volume   =  316,
  number   =  22,
  pages    = "2402--2410",
  month    =  dec,
  year     =  2016,
  language = "en"
}

@ARTICLE{Dunnmon2019-zw,
  title         = "{Cross-Modal} Data Programming Enables Rapid Medical Machine
                   Learning",
  author        = "Dunnmon, Jared and Ratner, Alexander and Khandwala, Nishith
                   and Saab, Khaled and Markert, Matthew and Sagreiya, Hersh
                   and Goldman, Roger and Lee-Messer, Christopher and Lungren,
                   Matthew and Rubin, Daniel and R{\'e}, Christopher",
  abstract      = "Labeling training datasets has become a key barrier to
                   building medical machine learning models. One strategy is to
                   generate training labels programmatically, for example by
                   applying natural language processing pipelines to text
                   reports associated with imaging studies. We propose
                   cross-modal data programming, which generalizes this
                   intuitive strategy in a theoretically-grounded way that
                   enables simpler, clinician-driven input, reduces required
                   labeling time, and improves with additional unlabeled data.
                   In this approach, clinicians generate training labels for
                   models defined over a target modality (e.g. images or time
                   series) by writing rules over an auxiliary modality (e.g.
                   text reports). The resulting technical challenge consists of
                   estimating the accuracies and correlations of these rules;
                   we extend a recent unsupervised generative modeling
                   technique to handle this cross-modal setting in a provably
                   consistent way. Across four applications in radiography,
                   computed tomography, and electroencephalography, and using
                   only several hours of clinician time, our approach matches
                   or exceeds the efficacy of physician-months of hand-labeling
                   with statistical significance, demonstrating a fundamentally
                   faster and more flexible way of building machine learning
                   models in medicine.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1903.11101"
}

@ARTICLE{Wang2019-jr,
  title    = "Real-time automatic detection system increases colonoscopic polyp
              and adenoma detection rates: a prospective randomised controlled
              study",
  author   = "Wang, Pu and Berzin, Tyler M and Glissen Brown, Jeremy Romek and
              Bharadwaj, Shishira and Becq, Aymeric and Xiao, Xun and Liu,
              Peixi and Li, Liangping and Song, Yan and Zhang, Di and Li, Yi
              and Xu, Guangre and Tu, Mengtian and Liu, Xiaogang",
  abstract = "OBJECTIVE: The effect of colonoscopy on colorectal cancer
              mortality is limited by several factors, among them a certain
              miss rate, leading to limited adenoma detection rates (ADRs). We
              investigated the effect of an automatic polyp detection system
              based on deep learning on polyp detection rate and ADR. DESIGN:
              In an open, non-blinded trial, consecutive patients were
              prospectively randomised to undergo diagnostic colonoscopy with
              or without assistance of a real-time automatic polyp detection
              system providing a simultaneous visual notice and sound alarm on
              polyp detection. The primary outcome was ADR. RESULTS: Of 1058
              patients included, 536 were randomised to standard colonoscopy,
              and 522 were randomised to colonoscopy with computer-aided
              diagnosis. The artificial intelligence (AI) system significantly
              increased ADR (29.1\%vs20.3\%, p<0.001) and the mean number of
              adenomas per patient (0.53vs0.31, p<0.001). This was due to a
              higher number of diminutive adenomas found (185vs102; p<0.001),
              while there was no statistical difference in larger adenomas
              (77vs58, p=0.075). In addition, the number of hyperplastic polyps
              was also significantly increased (114vs52, p<0.001). CONCLUSIONS:
              In a low prevalent ADR population, an automatic polyp detection
              system during colonoscopy resulted in a significant increase in
              the number of diminutive adenomas detected, as well as an
              increase in the rate of hyperplastic polyps. The cost-benefit
              ratio of such effects has to be determined further. TRIAL
              REGISTRATION NUMBER: ChiCTR-DDD-17012221; Results.",
  journal  = "Gut",
  month    =  feb,
  year     =  2019,
  keywords = "colonoscopy; colorectal cancer screening; computerised image
              analysis",
  language = "en"
}

@ARTICLE{Bien2018-ae,
  title    = "Deep-learning-assisted diagnosis for knee magnetic resonance
              imaging: Development and retrospective validation of {MRNet}",
  author   = "Bien, Nicholas and Rajpurkar, Pranav and Ball, Robyn L and Irvin,
              Jeremy and Park, Allison and Jones, Erik and Bereket, Michael and
              Patel, Bhavik N and Yeom, Kristen W and Shpanskaya, Katie and
              Halabi, Safwan and Zucker, Evan and Fanton, Gary and Amanatullah,
              Derek F and Beaulieu, Christopher F and Riley, Geoffrey M and
              Stewart, Russell J and Blankenberg, Francis G and Larson, David B
              and Jones, Ricky H and Langlotz, Curtis P and Ng, Andrew Y and
              Lungren, Matthew P",
  abstract = "BACKGROUND: Magnetic resonance imaging (MRI) of the knee is the
              preferred method for diagnosing knee injuries. However,
              interpretation of knee MRI is time-intensive and subject to
              diagnostic error and variability. An automated system for
              interpreting knee MRI could prioritize high-risk patients and
              assist clinicians in making diagnoses. Deep learning methods, in
              being able to automatically learn layers of features, are well
              suited for modeling the complex relationships between medical
              images and their interpretations. In this study we developed a
              deep learning model for detecting general abnormalities and
              specific diagnoses (anterior cruciate ligament [ACL] tears and
              meniscal tears) on knee MRI exams. We then measured the effect of
              providing the model's predictions to clinical experts during
              interpretation. METHODS AND FINDINGS: Our dataset consisted of
              1,370 knee MRI exams performed at Stanford University Medical
              Center between January 1, 2001, and December 31, 2012 (mean age
              38.0 years; 569 [41.5\%] female patients). The majority vote of 3
              musculoskeletal radiologists established reference standard
              labels on an internal validation set of 120 exams. We developed
              MRNet, a convolutional neural network for classifying MRI series
              and combined predictions from 3 series per exam using logistic
              regression. In detecting abnormalities, ACL tears, and meniscal
              tears, this model achieved area under the receiver operating
              characteristic curve (AUC) values of 0.937 (95\% CI 0.895,
              0.980), 0.965 (95\% CI 0.938, 0.993), and 0.847 (95\% CI 0.780,
              0.914), respectively, on the internal validation set. We also
              obtained a public dataset of 917 exams with sagittal T1-weighted
              series and labels for ACL injury from Clinical Hospital Centre
              Rijeka, Croatia. On the external validation set of 183 exams, the
              MRNet trained on Stanford sagittal T2-weighted series achieved an
              AUC of 0.824 (95\% CI 0.757, 0.892) in the detection of ACL
              injuries with no additional training, while an MRNet trained on
              the rest of the external data achieved an AUC of 0.911 (95\% CI
              0.864, 0.958). We additionally measured the specificity,
              sensitivity, and accuracy of 9 clinical experts (7
              board-certified general radiologists and 2 orthopedic surgeons)
              on the internal validation set both with and without model
              assistance. Using a 2-sided Pearson's chi-squared test with
              adjustment for multiple comparisons, we found no significant
              differences between the performance of the model and that of
              unassisted general radiologists in detecting abnormalities.
              General radiologists achieved significantly higher sensitivity in
              detecting ACL tears (p-value = 0.002; q-value = 0.019) and
              significantly higher specificity in detecting meniscal tears
              (p-value = 0.003; q-value = 0.019). Using a 1-tailed t test on
              the change in performance metrics, we found that providing model
              predictions significantly increased clinical experts' specificity
              in identifying ACL tears (p-value < 0.001; q-value = 0.006). The
              primary limitations of our study include lack of surgical ground
              truth and the small size of the panel of clinical experts.
              CONCLUSIONS: Our deep learning model can rapidly generate
              accurate clinical pathology classifications of knee MRI exams
              from both internal and external datasets. Moreover, our results
              support the assertion that deep learning models can improve the
              performance of clinical experts during medical imaging
              interpretation. Further research is needed to validate the model
              prospectively and to determine its utility in the clinical
              setting.",
  journal  = "PLoS Med.",
  volume   =  15,
  number   =  11,
  pages    = "e1002699",
  month    =  nov,
  year     =  2018,
  language = "en"
}

@INPROCEEDINGS{Zafar2017-ec,
  title     = "Fairness Beyond Disparate Treatment \& Disparate Impact:
               Learning Classification Without Disparate Mistreatment",
  booktitle = "Proceedings of the 26th International Conference on World Wide
               Web",
  author    = "Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez,
               Manuel and Gummadi, Krishna P",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "1171--1180",
  series    = "WWW '17",
  year      =  2017,
  address   = "Republic and Canton of Geneva, Switzerland",
  keywords  = "algorithmic decision making, discrimination in decision making,
               fair classification, fair decision making, machine learning and
               law",
  location  = "Perth, Australia"
}

@ARTICLE{Xie2016-ip,
  title         = "Aggregated Residual Transformations for Deep Neural Networks",
  author        = "Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and
                   Tu, Zhuowen and He, Kaiming",
  abstract      = "We present a simple, highly modularized network architecture
                   for image classification. Our network is constructed by
                   repeating a building block that aggregates a set of
                   transformations with the same topology. Our simple design
                   results in a homogeneous, multi-branch architecture that has
                   only a few hyper-parameters to set. This strategy exposes a
                   new dimension, which we call ``cardinality'' (the size of
                   the set of transformations), as an essential factor in
                   addition to the dimensions of depth and width. On the
                   ImageNet-1K dataset, we empirically show that even under the
                   restricted condition of maintaining complexity, increasing
                   cardinality is able to improve classification accuracy.
                   Moreover, increasing cardinality is more effective than
                   going deeper or wider when we increase the capacity. Our
                   models, named ResNeXt, are the foundations of our entry to
                   the ILSVRC 2016 classification task in which we secured 2nd
                   place. We further investigate ResNeXt on an ImageNet-5K set
                   and the COCO detection set, also showing better results than
                   its ResNet counterpart. The code and models are publicly
                   available online.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.05431"
}

@ARTICLE{Badgeley2019-zi,
  title    = "Deep learning predicts hip fracture using confounding patient and
              healthcare variables",
  author   = "Badgeley, Marcus A and Zech, John R and Oakden-Rayner, Luke and
              Glicksberg, Benjamin S and Liu, Manway and Gale, William and
              McConnell, Michael V and Percha, Bethany and Snyder, Thomas M and
              Dudley, Joel T",
  abstract = "Hip fractures are a leading cause of death and disability among
              older adults. Hip fractures are also the most commonly missed
              diagnosis on pelvic radiographs, and delayed diagnosis leads to
              higher cost and worse outcomes. Computer-aided diagnosis (CAD)
              algorithms have shown promise for helping radiologists detect
              fractures, but the image features underpinning their predictions
              are notoriously difficult to understand. In this study, we
              trained deep-learning models on 17,587 radiographs to classify
              fracture, 5 patient traits, and 14 hospital process variables.
              All 20 variables could be individually predicted from a
              radiograph, with the best performances on scanner model (AUC =
              1.00), scanner brand (AUC = 0.98), and whether the order was
              marked ``priority'' (AUC = 0.79). Fracture was predicted
              moderately well from the image (AUC = 0.78) and better when
              combining image features with patient data (AUC = 0.86, DeLong
              paired AUC comparison, p = 2e-9) or patient data plus hospital
              process features (AUC = 0.91, p = 1e-21). Fracture prediction on
              a test set that balanced fracture risk across patient variables
              was significantly lower than a random test set (AUC = 0.67,
              DeLong unpaired AUC comparison, p = 0.003); and on a test set
              with fracture risk balanced across patient and hospital process
              variables, the model performed randomly (AUC = 0.52, 95\% CI
              0.46-0.58), indicating that these variables were the main source
              of the model's fracture predictions. A single model that directly
              combines image features, patient, and hospital process data
              outperforms a Naive Bayes ensemble of an image-only model
              prediction, patient, and hospital process data. If CAD algorithms
              are inexplicably leveraging patient and process variables in
              their predictions, it is unclear how radiologists should
              interpret their predictions in the context of other known patient
              data. Further research is needed to illuminate deep-learning
              decision processes so that computers and clinicians can
              effectively cooperate.",
  journal  = "NPJ Digit Med",
  volume   =  2,
  pages    = "31",
  month    =  apr,
  year     =  2019,
  keywords = "Computer science; Radiography; Statistics",
  language = "en"
}

@MISC{Campanella2019-qs,
  title   = "Clinical-grade computational pathology using weakly supervised
             deep learning on whole slide images",
  author  = "Campanella, Gabriele and Hanna, Matthew G and Geneslaw, Luke and
             Miraflor, Allen and Silva, Vitor Werneck Krauss and Busam, Klaus J
             and Brogi, Edi and Reuter, Victor E and Klimstra, David S and
             Fuchs, Thomas J",
  journal = "Nature Medicine",
  volume  =  25,
  number  =  8,
  pages   = "1301--1309",
  year    =  2019
}

@ARTICLE{Taylor2018-ak,
  title    = "Automated detection of moderate and large pneumothorax on frontal
              chest X-rays using deep convolutional neural networks: A
              retrospective study",
  author   = "Taylor, Andrew G and Mielke, Clinton and Mongan, John",
  abstract = "BACKGROUND: Pneumothorax can precipitate a life-threatening
              emergency due to lung collapse and respiratory or circulatory
              distress. Pneumothorax is typically detected on chest X-ray;
              however, treatment is reliant on timely review of radiographs.
              Since current imaging volumes may result in long worklists of
              radiographs awaiting review, an automated method of prioritizing
              X-rays with pneumothorax may reduce time to treatment. Our
              objective was to create a large human-annotated dataset of chest
              X-rays containing pneumothorax and to train deep convolutional
              networks to screen for potentially emergent moderate or large
              pneumothorax at the time of image acquisition. METHODS AND
              FINDINGS: In all, 13,292 frontal chest X-rays (3,107 with
              pneumothorax) were visually annotated by radiologists. This
              dataset was used to train and evaluate multiple network
              architectures. Images showing large- or moderate-sized
              pneumothorax were considered positive, and those with trace or no
              pneumothorax were considered negative. Images showing small
              pneumothorax were excluded from training. Using an internal
              validation set (n = 1,993), we selected the 2 top-performing
              models; these models were then evaluated on a held-out internal
              test set based on area under the receiver operating
              characteristic curve (AUC), sensitivity, specificity, and
              positive predictive value (PPV). The final internal test was
              performed initially on a subset with small pneumothorax excluded
              (as in training; n = 1,701), then on the full test set (n =
              1,990), with small pneumothorax included as positive. External
              evaluation was performed using the National Institutes of Health
              (NIH) ChestX-ray14 set, a public dataset labeled for chest
              pathology based on text reports. All images labeled with
              pneumothorax were considered positive, because the NIH set does
              not classify pneumothorax by size. In internal testing, our
              ``high sensitivity model'' produced a sensitivity of 0.84 (95\%
              CI 0.78-0.90), specificity of 0.90 (95\% CI 0.89-0.92), and AUC
              of 0.94 for the test subset with small pneumothorax excluded. Our
              ``high specificity model'' showed sensitivity of 0.80 (95\% CI
              0.72-0.86), specificity of 0.97 (95\% CI 0.96-0.98), and AUC of
              0.96 for this set. PPVs were 0.45 (95\% CI 0.39-0.51) and 0.71
              (95\% CI 0.63-0.77), respectively. Internal testing on the full
              set showed expected decreased performance (sensitivity 0.55,
              specificity 0.90, and AUC 0.82 for high sensitivity model and
              sensitivity 0.45, specificity 0.97, and AUC 0.86 for high
              specificity model). External testing using the NIH dataset showed
              some further performance decline (sensitivity 0.28-0.49,
              specificity 0.85-0.97, and AUC 0.75 for both). Due to labeling
              differences between internal and external datasets, these
              findings represent a preliminary step towards external
              validation. CONCLUSIONS: We trained automated classifiers to
              detect moderate and large pneumothorax in frontal chest X-rays at
              high levels of performance on held-out test data. These models
              may provide a high specificity screening solution to detect
              moderate or large pneumothorax on images collected when human
              review might be delayed, such as overnight. They are not intended
              for unsupervised diagnosis of all pneumothoraces, as many small
              pneumothoraces (and some larger ones) are not detected by the
              algorithm. Implementation studies are warranted to develop
              appropriate, effective clinician alerts for the potentially
              critical finding of pneumothorax, and to assess their impact on
              reducing time to treatment.",
  journal  = "PLoS Med.",
  volume   =  15,
  number   =  11,
  pages    = "e1002697",
  month    =  nov,
  year     =  2018,
  language = "en"
}

@ARTICLE{Gale_W_Oakden-Rayner_L_Carneiro_G_Bradley_AP_Palmer_LJ2017-tl,
  title   = "Detecting hip fractures with radiologist-level performance using
             deep neural networks",
  author  = "{Gale W, Oakden-Rayner L, Carneiro G, Bradley AP, Palmer LJ}",
  journal = "arXiv",
  year    =  2017
}

@ARTICLE{Fries2019-ze,
  title    = "Weakly supervised classification of aortic valve malformations
              using unlabeled cardiac {MRI} sequences",
  author   = "Fries, Jason A and Varma, Paroma and Chen, Vincent S and Xiao, Ke
              and Tejeda, Heliodoro and Saha, Priyanka and Dunnmon, Jared and
              Chubb, Henry and Maskatia, Shiraz and Fiterau, Madalina and Delp,
              Scott and Ashley, Euan and R{\'e}, Christopher and Priest, James
              R",
  abstract = "Biomedical repositories such as the UK Biobank provide increasing
              access to prospectively collected cardiac imaging, however these
              data are unlabeled, which creates barriers to their use in
              supervised machine learning. We develop a weakly supervised deep
              learning model for classification of aortic valve malformations
              using up to 4,000 unlabeled cardiac MRI sequences. Instead of
              requiring highly curated training data, weak supervision relies
              on noisy heuristics defined by domain experts to programmatically
              generate large-scale, imperfect training labels. For aortic valve
              classification, models trained with imperfect labels
              substantially outperform a supervised model trained on
              hand-labeled MRIs. In an orthogonal validation experiment using
              health outcomes data, our model identifies individuals with a
              1.8-fold increase in risk of a major adverse cardiac event. This
              work formalizes a deep learning baseline for aortic valve
              classification and outlines a general strategy for using weak
              supervision to train machine learning models using unlabeled
              medical images at scale.",
  journal  = "Nat. Commun.",
  volume   =  10,
  number   =  1,
  pages    = "3111",
  month    =  jul,
  year     =  2019,
  language = "en"
}

@inproceedings{chen2019slicing,
  title={Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices},
  author={Chen, Vincent S and Wu, Sen and Weng, Zhenzhen and Ratner, Alexander and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2019},
  pdf={https://vincentsc.com/papers/chen2019slice.pdf}
}


@MISC{Liu2019-qt,
  title   = "A {Semi-Supervised} {CNN} Learning Method with Pseudo-class Labels
             for Atherosclerotic Vascular Calcification Detection",
  author  = "Liu, Jiamin and Yao, Jianhua and Bagheri, Mohammadhadi and
             Sandfort, Veit and Summers, Ronald M",
  journal = "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
             2019)",
  year    =  2019
}

@MISC{Yang_undated-bt,
  title       = "pytorch-classification",
  author      = "Yang, Wei",
  abstract    = "Classification with PyTorch. Contribute to
                 bearpaw/pytorch-classification development by creating an
                 account on GitHub.",
  institution = "Github"
}

@ARTICLE{Krizhevsky2009-tq,
  title   = "Cifar-10 and cifar-100 datasets",
  author  = "Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey",
  journal = "URl: https://www. cs. toronto. edu/kriz/cifar. html",
  volume  =  6,
  year    =  2009
}

@ARTICLE{Dunnmon2019-rr,
  title    = "Assessment of Convolutional Neural Networks for Automated
              Classification of Chest Radiographs",
  author   = "Dunnmon, Jared A and Yi, Darvin and Langlotz, Curtis P and
              R{\'e}, Christopher and Rubin, Daniel L and Lungren, Matthew P",
  abstract = "Purpose To assess the ability of convolutional neural networks
              (CNNs) to enable high-performance automated binary classification
              of chest radiographs. Materials and Methods In a retrospective
              study, 216 431 frontal chest radiographs obtained between 1998
              and 2012 were procured, along with associated text reports and a
              prospective label from the attending radiologist. This data set
              was used to train CNNs to classify chest radiographs as normal or
              abnormal before evaluation on a held-out set of 533 images
              hand-labeled by expert radiologists. The effects of development
              set size, training set size, initialization strategy, and network
              architecture on end performance were assessed by using standard
              binary classification metrics; detailed error analysis, including
              visualization of CNN activations, was also performed. Results
              Average area under the receiver operating characteristic curve
              (AUC) was 0.96 for a CNN trained with 200 000 images. This AUC
              value was greater than that observed when the same model was
              trained with 2000 images (AUC = 0.84, P .05). Averaging the CNN
              output score with the binary prospective label yielded the
              best-performing classifier, with an AUC of 0.98 (P < .005).
              Analysis of specific radiographs revealed that the model was
              heavily influenced by clinically relevant spatial regions but did
              not reliably generalize beyond thoracic disease. Conclusion CNNs
              trained with a modestly sized collection of prospectively labeled
              chest radiographs achieved high diagnostic performance in the
              classification of chest radiographs as normal or abnormal; this
              function may be useful for automated prioritization of abnormal
              chest radiographs. \copyright{} RSNA, 2018 Online supplemental
              material is available for this article. See also the editorial by
              van Ginneken in this issue.",
  journal  = "Radiology",
  volume   =  290,
  number   =  2,
  pages    = "537--544",
  month    =  feb,
  year     =  2019,
  language = "en"
}

@ARTICLE{Barocas2017-ka,
  title   = "Fairness in machine learning",
  author  = "Barocas, Solon and Hardt, Moritz and Narayanan, Arvind",
  journal = "NIPS Tutorial",
  year    =  2017
}

@ARTICLE{Buda2018-ab,
  title    = "A systematic study of the class imbalance problem in
              convolutional neural networks",
  author   = "Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A",
  abstract = "In this study, we systematically investigate the impact of class
              imbalance on classification performance of convolutional neural
              networks (CNNs) and compare frequently used methods to address
              the issue. Class imbalance is a common problem that has been
              comprehensively studied in classical machine learning, yet very
              limited systematic research is available in the context of deep
              learning. In our study, we use three benchmark datasets of
              increasing complexity, MNIST, CIFAR-10 and ImageNet, to
              investigate the effects of imbalance on classification and
              perform an extensive comparison of several methods to address the
              issue: oversampling, undersampling, two-phase training, and
              thresholding that compensates for prior class probabilities. Our
              main evaluation metric is area under the receiver operating
              characteristic curve (ROC AUC) adjusted to multi-class tasks
              since overall accuracy metric is associated with notable
              difficulties in the context of imbalanced data. Based on results
              from our experiments we conclude that (i) the effect of class
              imbalance on classification performance is detrimental; (ii) the
              method of addressing class imbalance that emerged as dominant in
              almost all analyzed scenarios was oversampling; (iii)
              oversampling should be applied to the level that completely
              eliminates the imbalance, whereas the optimal undersampling ratio
              depends on the extent of imbalance; (iv) as opposed to some
              classical machine learning models, oversampling does not cause
              overfitting of CNNs; (v) thresholding should be applied to
              compensate for prior class probabilities when overall number of
              properly classified cases is of interest.",
  journal  = "Neural Netw.",
  volume   =  106,
  pages    = "249--259",
  month    =  oct,
  year     =  2018,
  keywords = "Class imbalance; Convolutional neural networks; Deep learning;
              Image classification",
  language = "en"
}

@INPROCEEDINGS{Hardt2016-ac,
  title     = "Equality of opportunity in supervised learning",
  booktitle = "Advances in neural information processing systems",
  author    = "Hardt, Moritz and Price, Eric and Srebro, Nati and {Others}",
  pages     = "3315--3323",
  year      =  2016
}
